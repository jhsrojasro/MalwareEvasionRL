# https://github.com/vibalcam/ml-malware-detection <-- Datasets List
import torch as T
import torch.optim as optim
import numpy as np
from tqdm import tqdm
import copy
# import sys
# sys.path.append("..")
from ..utils.replay import MemoryReplay
from ..utils.valueFunctions import DuelingValueFunction, StateActionValueFunction

class DQNAgent:
    def __init__(self, env, nn_hidden_units = 256, nn_hidden_layers=2, memory_capacity = 50000, minibatch_size=32, epsilon_decay=False, verbose=True, checkpoint_path="models/dqn_cartpole_v0.pt"):
        super(DQNAgent, self).__init__()
        self.env = env
        self.nn_hidden_units = nn_hidden_units
        self.nn_hidden_layers = nn_hidden_layers
        self.memory_capacity = memory_capacity
        self.minibatch_size = minibatch_size
        self.epsilon = None
        self.verbose = verbose
        self.n_actions = self.env.action_space.n
        self.actions = T.arange(self.env.action_space.n)
        self.observation_dim = self.env.observation_space.shape[0] if len(env.observation_space.shape) == 1 else 1
        self.input_shape = (self.observation_dim,) # state 
        self.Q_value_function = DuelingValueFunction(self.input_shape, self.n_actions, self.nn_hidden_units, self.nn_hidden_layers)
        # self.Q_value_function = StateActionValueFunction(self.input_shape, self.n_actions, self.nn_hidden_units, self.nn_hidden_layers)
        self.memory = MemoryReplay(self.memory_capacity, self.observation_dim, self.minibatch_size)
        self.target_model = None
        self.epsilon_decay = epsilon_decay
        self.initial_epsilon = None
        self.final_epsilon = None
        self.checkpoint_path = checkpoint_path

    def choose_action(self, observation):
        if np.random.random() > self.epsilon:
            # print("Observation: ", observation.shape)
            state = T.tensor(observation, dtype=T.float).to(self.Q_value_function.device)
            values = self.Q_value_function.forward(state)
            action = T.argmax(values, dim=1).item()
            # print("Action: ", action)
            return action
        else:
            action = np.random.choice(self.env.action_space.n)
            return action

    def best_action(self, state):
        # state = T.tensor(state, dtype=T.float).to(self.Q_value_function.device)
        values = self.Q_value_function.forward(state)
        action = T.argmax(values, dim=1)
        return action

    def update_Q_value_function(self):
        batch = self.memory.sample()
        states = T.tensor(batch[:, :self.observation_dim], dtype=T.float).to(self.Q_value_function.device)
        actions = T.tensor(batch[:, self.observation_dim], dtype=T.long).to(self.Q_value_function.device)
        rewards = T.tensor(batch[:, self.observation_dim+1]).to(self.Q_value_function.device)
        next_states = T.tensor(batch[:, self.observation_dim+2:-1], dtype=T.float).to(self.Q_value_function.device)
        dones = T.tensor(batch[:, -1]).to(self.Q_value_function.device)
        
        outputs = self.Q_value_function.forward(states)

        outputs = outputs.gather(index=actions.view(-1,1), dim=1).view(-1)
        
        best_next_actions = self.best_action(next_states)
        next_actions_values = self.target_model.forward(next_states)
        next_actions_values = next_actions_values.gather(index=best_next_actions.view(-1,1), dim=1).view(-1)
        
        terminal_mask = 1 - dones 
        
        
        targets = rewards + terminal_mask * next_actions_values * self.gamma
        
        loss = T.mean((targets - outputs) ** 2)
        self.Q_value_function.optimizer.zero_grad()
        loss.backward()
        self.Q_value_function.optimizer.step()

    def train(self, n_steps_train=100000, start_step=1000, train_freq=1, learning_rate=5e-4,
               gamma=0.99, frozen_target_delay=500, epsilon = 0.15,
                initial_epsilon=0.15, final_epsilon=0.01, print_delay=100, load_model=True):
        if self.epsilon_decay:
            self.initial_epsilon = initial_epsilon
            self.final_epsilon = final_epsilon
        else:
            self.epsilon = epsilon
        self.learning_rate = learning_rate
        self.gamma = gamma
        self.memory.clear()
        self.Q_value_function.optimizer = optim.Adam(self.Q_value_function.parameters(), lr=learning_rate)
        
        metrics = []
        rewards = []
        n_steps_per_episode = []
        n_episodes = 0
        best_mean_reward = 0.0

        self.target_model = copy.deepcopy(self.Q_value_function)
        reset = self.env.reset()
        if isinstance(reset, tuple):
            state = reset[0]
        else:
            state = reset
        n_steps_this_episode = 0
        done = False
        trunc = False
        
        for t in tqdm(range(n_steps_train), desc="Training Agent..."):
            self.epsilon = max(final_epsilon, initial_epsilon - t / n_steps_train * (initial_epsilon - final_epsilon)) \
                if self.epsilon_decay else self.epsilon
            
            action = self.choose_action(state)
            obs = self.env.step(action)
            if len(obs) == 5:
                next_state, reward, done, trunc, info = obs
            elif len(obs) == 4:
                next_state, reward, done, info = obs
            self.memory.push(state, action, reward, next_state, done)
            n_steps_this_episode += 1
            
            if t >= start_step and t % train_freq == 0:
                self.update_Q_value_function()
            
            if t >= start_step and t % frozen_target_delay == 0:
                self.target_model = copy.deepcopy(self.Q_value_function)
            
            if done or trunc:
                state = self.env.reset()[0]
                done = False
                rewards.append(reward * n_steps_this_episode)
                n_steps_per_episode.append(n_steps_this_episode)
                n_episodes += 1
                n_steps_this_episode = 0
                mean_reward = round(np.mean(rewards[-101:-1]), 1)

                if mean_reward > best_mean_reward:
                    best_mean_reward = mean_reward
                    if self.verbose:
                        print(f"Saving model with new best mean reward: {best_mean_reward}")
                    T.save(self.Q_value_function, self.checkpoint_path)

                if n_episodes % print_delay == 0:
                    # print("Rewards: ", rewards)
                    if n_episodes == 1:
                        mean_reward = rewards[-1]
                        std_reward = 0.0
                        median_reward = rewards[-1]
                        min_reward = rewards[-1]
                        max_reward = rewards[-1]
                        mean_steps = n_steps_per_episode[-1]
                    else:
                        n_eps = max(-(n_episodes+1), -101)
                        mean_reward = round(np.mean(rewards[n_eps:-1]), 1)
                        std_reward = round(np.std(rewards[n_eps:-1]), 1)
                        median_reward = round(np.median(rewards[n_eps:-1]), 1)
                        min_reward = round(np.min(rewards[n_eps:-1]), 1) 
                        max_reward = round(np.max(rewards[n_eps:-1]), 1) 
                        mean_steps = round(np.mean(n_steps_per_episode[n_eps:-1]), 1)
                    # print(f"Episode: {n_episodes}, Mean Reward: {mean_reward}")
                    metrics.append((mean_reward, std_reward, median_reward, min_reward, max_reward, mean_steps))
            
            state = next_state

        # Model class must be defined somewhere
        if load_model:
            print("Loading model from checkpoint: ", self.checkpoint_path, "with best mean reward: ", best_mean_reward)
            self.Q_value_function = T.load(self.checkpoint_path)
            self.Q_value_function.eval()

        return np.array(metrics)
    
    def test(self, n_episodes_test, verbose=True):
        with T.no_grad():
            rewards = np.zeros(n_episodes_test)
            mean_steps = np.zeros(n_episodes_test)
            for i in tqdm(range(n_episodes_test), desc="Testing Agent...") if verbose else range(n_episodes_test):
                reset = self.env.reset()
                if isinstance(reset, tuple):
                    state = reset[0]
                else:
                    state = reset
                done = False
                trunc = False
                total_reward = 0
                steps = 0
                while not (done or trunc):
                    action = self.choose_action(state)
                    obs = self.env.step(action)
                    if len(obs) == 5:
                        next_state, reward, done, trunc ,info = obs
                    elif len(obs) == 4:
                        next_state, reward, done, info = obs
                    total_reward += reward
                    steps += 1
                    state = next_state
                rewards[i] = total_reward
                mean_steps[i] = steps
        mean_reward = np.mean(rewards)
        std_reward = np.std(rewards)
        median_reward = np.median(rewards)
        min_reward = np.min(rewards)
        max_reward = np.max(rewards)
        mean_steps = np.mean(mean_steps)
        return mean_reward, std_reward, median_reward, min_reward, max_reward, mean_steps

class DQNAgentMalwareEvasion(DQNAgent):
    def test(self, n_episodes_test=100):
        evasions = 0
        evasion_history = {}
        for _ in tqdm(range(n_episodes_test), desc="Testing DQN Agent..."):
            ob = self.env.reset()
            sha256 = self.env.sha256
            while True:
                action = self.choose_action(ob)
                ob, reward, done, ep_history = self.env.step(action)
                if done and reward >= 10.0:
                    evasions += 1
                    evasion_history[sha256] = ep_history
                    break

                elif done:
                    break
        return evasions, evasion_history