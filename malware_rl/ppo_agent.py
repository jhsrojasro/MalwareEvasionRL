import gym
import malware_rl
import sys
import pickle
import numpy as np
from tqdm import tqdm
import matplotlib.pyplot as plt
import argparse
sys.path.append("..")
from RL.PPO.ppo import PPOAgent 
from RL.utils.graphs import plot_metrics

default_train_episodes = 10
default_epochs = 10
default_test_episodes = 10
default_batch_size = 8

parser = argparse.ArgumentParser()
parser.add_argument("--train_episodes", type=int, default=default_train_episodes, help="Number of training episodes")
parser.add_argument("--epochs", type=int, default=10, help="Number of training epochs")
parser.add_argument("--train_delay", type=int, default=10, help="Number of steps to wait between training")
parser.add_argument("--test_episodes", type=int, default=default_test_episodes, help="Number of test episodes")
parser.add_argument("--environment", type=str, default="ember-train-v0", help="Environment to train the agent: ember or malconv")
parser.add_argument("--alpha", type=float, default=5e-4, help="Learning rate to train the Networks")
parser.add_argument("--policy_clip", type=float, default=0.2, help="Clip parameter for the PPO algorithm")
parser.add_argument("--gamma", type=float, default=0.99, help="Discount factor")
parser.add_argument("--lambda_gae", type=float, default=0.95, help="Lambda parameter for Generalized Advantage Estimation")
# parser.add_argument("--hidden_units", type=int, default=64, help="Number of hidden units in the neural network")
parser.add_argument("--batch_size", type=int, default=default_batch_size, help="Batch size")
# parser.add_argument("--hidden_layers", type=int, default=2, help="Number of hidden layers in the neural network")

args = parser.parse_args()

selected_environment = None

if args.environment == "ember" or args.environment == "ember-train-v0":
    selected_environment =  "ember-train-v0"
elif args.environment == "malconv":
    selected_environment = "malconv-train-v0"
else:
    print("Invalid environment")
    exit()

epochs = args.epochs
N = args.train_delay
train_episodes = args.train_episodes
test_episodes = args.test_episodes
alpha = args.alpha
gamma = args.gamma
policy_clip = args.policy_clip
lambda_gae = args.lambda_gae
# n_hidden_units = args.hidden_units
# n_hidden_layers = args.hidden_layers
batch_size = args.batch_size

# epochs = 2
# test_episodes = 3
# batch_size = 2

# environments = [ "malconv-train-v0", "ember-train-v0"]

env = gym.make(selected_environment)

agent = PPOAgent(
    n_actions=env.action_space.n, 
    batch_size=batch_size,
    alpha=alpha, 
    n_epochs=epochs,                
    input_dims=env.observation_space.shape
)
train_episodes = 10
# n_games = 10
# N = 
figure_file = 'plots/ppo.png'

best_score = env.reward_range[0]
score_history = []
means_scores = []
learn_iters = 0
avg_score = 0
n_steps = 0

for i in tqdm(range(train_episodes), desc='Training PPO...'):
    observation = env.reset()
    done = False
    score = 0
    while not done:
        action, prob, val = agent.choose_action(observation)
        observation_, reward, done, info = env.step(action)
        n_steps += 1
        score += reward
        agent.remember(observation, action, prob, val, reward, done)
        if n_steps % N == 0:
            agent.learn()
            learn_iters += 1
        observation = observation_
        score_history.append(score)
        avg_score = np.mean(score_history[-100:])


        if avg_score > best_score:
            best_score = avg_score
            agent.save_models()
    means_scores.append(avg_score)
        # print('episode ', i, 'score %.1f' % score, 'avg score %.1f' % avg_score, 'time_steps', n_steps, 'learning_steps', learn_iters)

plt.plot(range(train_episodes), means_scores)
plt.savefig(f"plots/ppo_ember_{train_episodes}_episodes.png")

evasions = 0
evasion_history = {}
for _ in tqdm(range(test_episodes), desc="Testing PPO Agent..."):
    ob = env.reset()
    sha256 = env.sha256
    while True:
        action, _, _ = agent.choose_action(ob)
        ob, reward, done, ep_history = env.step(action)
        if done and reward >= 10.0:
            evasions += 1
            evasion_history[sha256] = ep_history
            break

        elif done:
            break
print("Number of evasions: ", evasions)
print("Percentage of evasions: ", evasions/test_episodes)
pickle.dump( evasion_history, open( f"data/evaded/ember/ppo_ember_{train_episodes}_episodes.p", "wb" ) )
pickle.dump( agent, open( f"models/ppo_ember_{train_episodes}_episodes.p", "wb" ))