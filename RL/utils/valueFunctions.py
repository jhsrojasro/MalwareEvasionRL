import torch as T
import torch.nn as nn
import torch.optim as optim

class DuelingValueFunction(nn.Module):
    def __init__(self, input_dims, n_actions, nn_hidden_units = 64, nn_hidden_layers=1 ):
        super(DuelingValueFunction, self).__init__()
        self.input_dims = input_dims
        self.nn_hidden_units = nn_hidden_units
        self.n_actions = n_actions
        self.Advantages = nn.Sequential(
            nn.Linear(*self.input_dims, self.nn_hidden_units),
            *([nn.Linear(self.nn_hidden_units, self.nn_hidden_units),
            nn.ReLU() ] * nn_hidden_layers ),
            nn.Linear(self.nn_hidden_units, n_actions)
        )
        self.StateValue = nn.Sequential(
            nn.Linear(*self.input_dims, self.nn_hidden_units),
            *([nn.Linear(self.nn_hidden_units, self.nn_hidden_units),
            nn.ReLU() ] * nn_hidden_layers ),
            nn.Linear(self.nn_hidden_units, 1)
        )
        self.device = T.device("cuda:0" if T.cuda.is_available() else "cpu")
        # self.optimizer = optim.Adam(self.model.parameters(), learning_rate=self.learning_rate)
        self.to(self.device)
        # self.build_model()

    def forward(self, state):
        advantages = self.Advantages(state)
        if advantages.dim() == 1:
            advantages = advantages.unsqueeze(0)

        state_value = self.StateValue(state)
        advantages_values_mean = T.mean(advantages, dim=1)
        actions_values_centered = advantages - advantages_values_mean.view(-1,1)
        q_values = state_value + actions_values_centered 
        return q_values

class StateActionValueFunction(nn.Module):
    def __init__(self, input_dims, n_actions, nn_hidden_units = 64, nn_hidden_layers=1 ):
        super(StateActionValueFunction, self).__init__()
        self.input_dims = input_dims
        self.state_dims = (input_dims[0] - 1,)
        self.nn_hidden_units = nn_hidden_units
        self.n_actions = n_actions
        self.StateActionValue = nn.Sequential(
            nn.Linear(*self.input_dims, self.nn_hidden_units),
            *([nn.Linear(self.nn_hidden_units, self.nn_hidden_units),
            nn.ReLU() ] * nn_hidden_layers ),
            nn.Linear(self.nn_hidden_units, 1)
        )
        self.device = T.device("cuda:0" if T.cuda.is_available() else "cpu")
        self.to(self.device)

    def forward(self, input):
        return self.StateActionValue(input)

class ActionValueFunction(nn.Module):
    def __init__(self,input_dims, n_actions, nn_hidden_units = 64, nn_hidden_layers=1):
        super(ActionValueFunction, self).__init__()
        self.input_dims = input_dims
        self.nn_hidden_units = nn_hidden_units
        self.n_actions = n_actions
        self.nn_hidden_layers = nn_hidden_layers
        self.ActionValue = nn.Sequential(
            nn.Linear(*self.input_dims, self.nn_hidden_units),
            *([nn.Linear(self.nn_hidden_units, self.nn_hidden_units),
            nn.ReLU() ] * nn_hidden_layers ),
            nn.Linear(self.nn_hidden_units, self.n_actions)
        )
        self.device = T.device("cuda:0" if T.cuda.is_available() else "cpu")
        self.to(self.device)
    
    def forward(self, input):
        return self.ActionValue(input)
    
    
class ValueFunction(nn.Module):
    def __init__(self, input_dims, n_hidden_layer=2, n_hidden_neurons=64 ):
        super(ValueFunction, self).__init__()
        self.input_dims = input_dims
        self.n_hidden_layer = n_hidden_layer
        self.n_hidden_neurons = n_hidden_neurons
        self.value = nn.Sequential(
            nn.Linear(*self.input_dims, self.n_hidden_neurons),
            *([nn.Linear(self.n_hidden_neurons, self.n_hidden_neurons),
            nn.ReLU() ] * self.n_hidden_layer ),
            nn.Linear(self.n_hidden_neurons, 1),
        )
        self.device = T.device("cuda:0" if T.cuda.is_available() else "cpu")
        self.to(self.device)
    
    def forward(self, state):
        value = self.value(state)
        return value

class ValueFunctionLinearRegression(nn.Module):
    def __init__(self, inputDims):
        super(ValueFunctionLinearRegression, self).__init__()
        self.inputDims = inputDims
        self.numFeatures = inputDims[0]
        self.linear = nn.Linear(self.numFeatures, 1)
    
    def forward(self, x):
        out  = self.linear(x)
        return out
    