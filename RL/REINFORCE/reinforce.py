import torch as T
import torch.nn as nn
import torch.optim as optim
from torch.distributions.categorical import Categorical
import numpy as np
from tqdm import tqdm
import sys
sys.path.append("../")
# from utils.replay import ReplayMemory, Transition, plot_learning_curve
from utils.valueFunctions import ValueFunctionLinearRegression

class PolicyNetwork(nn.Module):
    def __init__(self, input_dims, n_actions, nn_hidden_units = 64, nn_hidden_layers=1 ):
        super(PolicyNetwork, self).__init__()
        self.input_dims = input_dims
        self.nn_hidden_units = nn_hidden_units
        self.n_actions = n_actions
        self.policy = nn.Sequential(
            nn.Linear(*self.input_dims, self.nn_hidden_units),
            *([nn.Linear(self.nn_hidden_units, self.nn_hidden_units),
            nn.ReLU() ] * nn_hidden_layers ),
            nn.Linear(self.nn_hidden_units, n_actions),
            nn.Softmax(dim=-1)
        )
        self.device = T.device("cuda:0" if T.cuda.is_available() else "cpu")
        self.to(self.device)

    def forward(self, state):
        dist = self.policy(state)
        dist = Categorical(dist)
        return dist


class ReinforceAgent:
    def __init__(self, env, input_dims, n_actions, nn_hidden_units = 64, nn_hidden_layers=1):
        self.env = env
        self.gamma = None
        self.policy_lr = None
        self.value_lr = None
        self.input_dims = input_dims
        self.nn_hidden_units = nn_hidden_units
        self.nn_hidden_layers = nn_hidden_layers
        self.n_actions = n_actions
        self.policy = PolicyNetwork(input_dims, n_actions, nn_hidden_units, nn_hidden_layers)
        self.valueFunction = ValueFunctionLinearRegression(input_dims)
        self.policyOptimizer = None
        self.valueOptimizer = None
        self.device = T.device("cuda:0" if T.cuda.is_available() else "cpu")

    def choose_action(self, observation):
        # print("Observation shape: ", observation.shape)
        state = T.tensor(observation, dtype=T.float).to(self.device)
        dist = self.policy(state)
        action = dist.sample()
        # log_prob = dist.log_prob(action)
        return action.item()
    
    def play_episode(self):
        done = False
        trunc = False
        state = self.env.reset()[0]
        rewards = []
        states = []
        actions = []
        dones = []
        next_states = []
        while not (done or trunc):
            action = self.choose_action(state)
            next_state, reward, done, trunc, info = self.env.step(action)
            states.append(state)
            actions.append(action)
            rewards.append(reward)
            dones.append(done)
            next_states.append(next_state)
            state = next_state
        states = T.tensor(np.array(states), dtype=T.float, device=self.device).detach()
        actions = T.tensor(actions, dtype=T.long, device=self.device).detach()
        rewards = T.tensor(rewards, dtype=T.float, device=self.device).detach()
        dones = T.tensor(dones, dtype=T.long, device=self.device).detach()
        next_states = T.tensor(np.array(next_states), dtype=T.float, device=self.device).detach()
        
        return states, actions, rewards, dones, next_states
    
    def collect_trajectories(self, n_trajectories=100, verbose=False):
        rewards = []
        actions = []
        states = []
        dones = []
        next_states = []
        for _ in range(n_trajectories) if not verbose else tqdm(range(n_trajectories), desc="Collecting trajectories..."):
            sts, acts, rwds, dns, nsts = self.play_episode()
            rewards.append(rwds)
            actions.append(acts)
            states.append(sts)
            dones.append(dns)
            next_states.append(nsts)
        return states, actions, rewards, dones, next_states
    
    def rewards_to_go(self, rewards_trajectories):
        rewards_to_go = []
        for rewards in rewards_trajectories:
            rewards_to_go.append(T.cumsum(rewards.flip(dims=(0,)), dim=0 ).flip(dims=(0,)).detach())
        return rewards_to_go

    def compute_advantages(self, state_values, rewards_to_go, dones, next_states_values):
        advantages = []
        for i in range(len(rewards_to_go)):
            trajectory_length = len(rewards_to_go[i])
            advs = np.zeros(trajectory_length)
            for t in range(trajectory_length):
                discount = 1
                a_t = 0
                for k in range(t, trajectory_length):
                    a_t += discount * (rewards_to_go[i][k] + self.gamma * next_states_values[i][k] * (1 - dones[i][k]) - state_values[i][k])
                    discount *= self.gamma * self.lambda_gae
                advs[t] = a_t
            advantages.append(T.tensor(advs, dtype=T.float, device=self.device).detach())
        return advantages

    def train(self, n_epochs=100, policy_learning_rate=5e-04, value_learning_rate=5e-04 ,gamma=0.99,
               lambda_gae=0.95, batch_size=1000, save_path="/models/cartpole_v1_reinforce.pt", verbose=True):
        self.policy = PolicyNetwork(self.input_dims, self.n_actions, self.nn_hidden_units, self.nn_hidden_layers)
        self.valueFunction = ValueFunctionLinearRegression(self.input_dims)
        self.lambda_gae = lambda_gae
        self.gamma = gamma
        self.policy_lr = policy_learning_rate
        self.value_lr = value_learning_rate
        self.policyOptimizer = optim.Adam(self.policy.parameters(), lr=self.policy_lr)
        self.valueOptimizer = optim.Adam(self.valueFunction.parameters(), lr=self.value_lr)
        self.valueCriterion = nn.MSELoss()

        for epoch in range(n_epochs) if not verbose else tqdm(range(n_epochs), desc="Training Agent..."):
            states, actions, rewards, dones, next_states = self.collect_trajectories(batch_size)
            rewards_to_go = self.rewards_to_go(rewards)
            states_values = []
            next_states_values =[]
            dist = []
            log_probs = []
            for i in range(batch_size):
                states_values.append(self.valueFunction(states[i]))
                next_states_values.append(self.valueFunction(next_states[i]))
                dist.append(self.policy(states[i]))
                log_probs.append(dist[i].log_prob(actions[i]))
            advantages = self.compute_advantages(states_values, rewards_to_go, dones, next_states_values)
            
            policy_losses = T.zeros(batch_size, device=self.device)
            value_losses = T.zeros(batch_size, device=self.device)

            for i in range(batch_size):
                policy_losses[i] = (log_probs[i] * advantages[i]).sum()
                # print("States values shape: ", states_values[i].squeeze().shape)
                # print("Rewards to go shape: ", rewards_to_go[i].shape)
                value_losses[i] = self.valueCriterion(states_values[i].squeeze(), rewards_to_go[i])
            policy_loss = -policy_losses.mean()
            value_loss = value_losses.mean()

            self.policyOptimizer.zero_grad()
            policy_loss.backward()
            self.policyOptimizer.step()

            self.valueOptimizer.zero_grad()
            value_loss.backward()
            self.valueOptimizer.step()

            # dist = self.policy(states)
            # log_probs = dist.log_prob(actions)
            # policy_losses = -log_probs * advantages
            # policy_loss = policy_losses.mean()
            metrics = self.test()
            # print(f"Policy loss epoch {epoch}: ", policy_loss.item())
            # print(f"Value loss epoch {epoch}: ", value_loss.item())
            print(f"Metrics epoch {epoch}: ", metrics)
    
    def test(self, n_episodes_test = 100, verbose=False):
        rewards = np.zeros(n_episodes_test)
        for i in range(n_episodes_test) if not verbose else tqdm(range(n_episodes_test), desc="Testing Agent..."):
            _, _, rwds, _, _ = self.play_episode()
            rewards[i] = sum(rwds)
        mean_reward = np.mean(rewards)
        std_reward = np.std(rewards)
        median_reward = np.median(rewards)
        max_reward = np.max(rewards)
        min_reward = np.min(rewards)
        return mean_reward, std_reward, median_reward, max_reward, min_reward
