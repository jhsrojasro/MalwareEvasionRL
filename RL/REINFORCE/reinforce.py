import torch as T
import torch.nn as nn
import torch.optim as optim
from torch.distributions.categorical import Categorical
import numpy as np
from tqdm import tqdm
# from utils.valueFunctions import ValueFunctionLinearRegression
# import sys
# sys.path.append("../")
# from utils.replay import ReplayMemory, Transition, plot_learning_curve

class PolicyNetwork(nn.Module):
    def __init__(self, input_dims, n_actions, nn_hidden_units = 64, nn_hidden_layers=1 ):
        super(PolicyNetwork, self).__init__()
        self.input_dims = input_dims
        self.nn_hidden_units = nn_hidden_units
        self.n_actions = n_actions
        self.policy = nn.Sequential(
            nn.Linear(*self.input_dims, self.nn_hidden_units),
            *([nn.Linear(self.nn_hidden_units, self.nn_hidden_units),
            nn.ReLU() ] * nn_hidden_layers ),
            nn.Linear(self.nn_hidden_units, n_actions),
            nn.Softmax(dim=-1)
        )
        self.device = T.device("cuda:0" if T.cuda.is_available() else "cpu")
        self.to(self.device)

    def forward(self, state):
        dist = self.policy(state)
        dist = Categorical(dist)
        return dist

class ValueFunctionLinearRegression(nn.Module):
    def __init__(self, inputDims):
        super(ValueFunctionLinearRegression, self).__init__()
        self.inputDims = inputDims
        self.numFeatures = inputDims[0]
        self.linear = nn.Linear(self.numFeatures, 1)
    
    def forward(self, x):
        out  = self.linear(x)
        return out


class ReinforceAgent:
    def __init__(self, env, nn_hidden_units = 64, nn_hidden_layers=1):
        self.env = env
        self.gamma = None
        self.policy_lr = None
        self.value_lr = None
        self.observation_dim = self.env.observation_space.shape[0] if len(env.observation_space.shape) == 1 else 1
        self.input_shape = (self.observation_dim,)
        self.nn_hidden_units = nn_hidden_units
        self.nn_hidden_layers = nn_hidden_layers
        self.n_actions = self.env.action_space.n
        self.policy = PolicyNetwork(self.input_shape, self.n_actions, nn_hidden_units, nn_hidden_layers)
        self.valueFunction = ValueFunctionLinearRegression(self.input_shape)
        self.policyOptimizer = None
        self.valueOptimizer = None
        # self.epsilon_decay = epsilon_decay
        self.device = T.device("cuda:0" if T.cuda.is_available() else "cpu")

    def choose_action(self, observation):
        # print("Observation shape: ", observation.shape)
        state = T.tensor(observation, dtype=T.float).to(self.device)
        dist = self.policy(state)
        action = dist.sample()
        # log_prob = dist.log_prob(action)
        return action.item()
    
    def play_episode(self):
        done = False
        trunc = False
        reset = self.env.reset()
        if isinstance(reset, tuple):
            state = reset[0]
        else:
            state = reset
        rewards = []
        states = []
        actions = []
        dones = []
        next_states = []
        while not (done or trunc):
            action = self.choose_action(state)
            obs = self.env.step(action)
            if len(obs) == 5:
                next_state, reward, done, trunc, info = obs
            elif len(obs) == 4:
                next_state, reward, done, info = obs
            states.append(state)
            actions.append(action)
            rewards.append(reward)
            dones.append(done)
            next_states.append(next_state)
            state = next_state
        states = T.tensor(np.array(states), dtype=T.float, device=self.device).detach()
        actions = T.tensor(actions, dtype=T.long, device=self.device).detach()
        rewards = T.tensor(rewards, dtype=T.float, device=self.device).detach()
        dones = T.tensor(dones, dtype=T.long, device=self.device).detach()
        next_states = T.tensor(np.array(next_states), dtype=T.float, device=self.device).detach()
        
        return states, actions, rewards, dones, next_states
    
    def collect_trajectories(self, n_trajectories=100, verbose=False):
        rewards = []
        actions = []
        states = []
        dones = []
        next_states = []
        for _ in range(n_trajectories) if not verbose else tqdm(range(n_trajectories), desc="Collecting trajectories..."):
            sts, acts, rwds, dns, nsts = self.play_episode()
            rewards.append(rwds)
            actions.append(acts)
            states.append(sts)
            dones.append(dns)
            next_states.append(nsts)
        return states, actions, rewards, dones, next_states
    
    def rewards_to_go(self, rewards_trajectories):
        rewards_to_go = []
        for rewards in rewards_trajectories:
            rewards_to_go.append(T.cumsum(rewards.flip(dims=(0,)), dim=0 ).flip(dims=(0,)).detach())
        return rewards_to_go

    def compute_advantages(self, state_values, rewards_to_go, dones, next_states_values):
        advantages = []
        for i in range(len(rewards_to_go)):
            trajectory_length = len(rewards_to_go[i])
            advs = np.zeros(trajectory_length)
            for t in range(trajectory_length):
                discount = 1
                a_t = 0
                for k in range(t, trajectory_length):
                    a_t += discount * (rewards_to_go[i][k] + self.gamma * next_states_values[i][k] * (1 - dones[i][k]) - state_values[i][k])
                    discount *= self.gamma * self.lambda_gae
                advs[t] = a_t
            advantages.append(T.tensor(advs, dtype=T.float, device=self.device).detach())
        return advantages

    def train(self, n_epochs=100, policy_learning_rate=5e-04, value_learning_rate=5e-04 ,gamma=0.99,
               lambda_gae=0.95, batch_size=1000, save_path="/models/cartpole_v1_reinforce.pt", verbose=True):
        self.policy = PolicyNetwork(self.input_shape, self.n_actions, self.nn_hidden_units, self.nn_hidden_layers)
        self.valueFunction = ValueFunctionLinearRegression(self.input_shape)
        self.lambda_gae = lambda_gae
        self.gamma = gamma
        self.policy_lr = policy_learning_rate
        self.value_lr = value_learning_rate
        self.policyOptimizer = optim.Adam(self.policy.parameters(), lr=self.policy_lr)
        self.valueOptimizer = optim.Adam(self.valueFunction.parameters(), lr=self.value_lr)
        self.valueCriterion = nn.MSELoss()
        best_evasion_rate = -1.0
        train_metrics = []

        for epoch in range(n_epochs) if not verbose else tqdm(range(n_epochs), desc="Training Agent..."):
            # print("Starting epoch: ", epoch)
            states, actions, rewards, dones, next_states = self.collect_trajectories(batch_size)
            
            # Collect metrics of interaction this epoch
            final_rewards = np.array([ reward_tensor[-1].item() for reward_tensor in rewards])
            n_evasions = sum([ reward_tensor[-1].item() >= 10 for reward_tensor in rewards])
            evasion_rate = n_evasions / batch_size
            mean_reward = final_rewards.mean()
            std_reward = final_rewards.std()
            median_reward = np.median(final_rewards)
            max_reward = np.max(final_rewards)
            min_reward = np.min(final_rewards)
            train_metrics.append([evasion_rate, mean_reward, std_reward, median_reward, max_reward, min_reward])
            
            if verbose:
                print(f"Evasion rate epoch {epoch}:", evasion_rate)
            
            # Save model if better evasion rate
            if evasion_rate > best_evasion_rate:
                best_evasion_rate = evasion_rate
                if verbose:
                    print("Saving model with best evasion rate: ", evasion_rate)
                T.save(self.policy, save_path)
            
            
            rewards_to_go = self.rewards_to_go(rewards)
            # print("Mean Rewards: ", [ r.item()  for reward in rewards for r in reward.mean()])
            states_values = []
            next_states_values =[]
            dist = []
            log_probs = []

            # rewards = []

            for i in range(batch_size):
                states_values.append(self.valueFunction(states[i]))
                next_states_values.append(self.valueFunction(next_states[i]))
                dist.append(self.policy(states[i]))
                log_probs.append(dist[i].log_prob(actions[i]))
            advantages = self.compute_advantages(states_values, rewards_to_go, dones, next_states_values)
            
            policy_losses = T.zeros(batch_size, device=self.device)
            value_losses = T.zeros(batch_size, device=self.device)

            for i in range(batch_size):
                policy_losses[i] = (log_probs[i] * advantages[i]).sum()
                # print("States values shape: ", states_values[i].squeeze().shape)
                # print("Rewards to go shape: ", rewards_to_go[i].shape)
                value_losses[i] = self.valueCriterion(states_values[i].squeeze(), rewards_to_go[i])
            policy_loss = -policy_losses.mean()
            value_loss = value_losses.mean()

            self.policyOptimizer.zero_grad()
            policy_loss.backward()
            self.policyOptimizer.step()

            self.valueOptimizer.zero_grad()
            value_loss.backward()
            self.valueOptimizer.step()


        return np.array(train_metrics)
            # print(f"Metrics epoch {epoch}: ", metrics)
    
    def test(self, n_episodes_test = 100, verbose=False):
        rewards = np.zeros(n_episodes_test)
        for i in range(n_episodes_test) if not verbose else tqdm(range(n_episodes_test), desc="Testing Agent..."):
            _, _, rwds, _, _ = self.play_episode()
            rewards[i] = sum(rwds)
        mean_reward = np.mean(rewards)
        std_reward = np.std(rewards)
        median_reward = np.median(rewards)
        max_reward = np.max(rewards)
        min_reward = np.min(rewards)
        return mean_reward, std_reward, median_reward, max_reward, min_reward


class ReinforceMalwareAgent(ReinforceAgent):
    def test(self, n_episodes_test=100):
        evasions = 0
        evasion_history = {}
        for _ in tqdm(range(n_episodes_test), desc="Testing DQN Agent..."):
            ob = self.env.reset()
            sha256 = self.env.sha256
            while True:
                action = self.choose_action(ob)
                ob, reward, done, ep_history = self.env.step(action)
                if done and reward >= 10.0:
                    evasions += 1
                    evasion_history[sha256] = ep_history
                    break

                elif done:
                    break
        return evasions, evasion_history